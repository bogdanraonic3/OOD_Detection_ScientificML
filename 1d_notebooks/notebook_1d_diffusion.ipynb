{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.stats import gaussian_kde\n",
    "import tqdm\n",
    "import functools\n",
    "from scipy import integrate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim = 1,\n",
    "                 out_dim = 1,\n",
    "                 widths = [64, 128, 128, 64]):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        widths = [in_dim] + list(widths) + [out_dim]\n",
    "        self.n_layers = len(widths) - 1\n",
    "        self.layers = nn.ModuleList([nn.Linear(widths[i], widths[i+1]) for i in range(self.n_layers)])\n",
    "        #self.activation = Swish()#nn.ReLU()\n",
    "        self.activation = nn.LeakyReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers - 1):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.activation(x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "#----------------------------\n",
    "\n",
    "def train(model, optimizer, scheduler, training_loader, val_loader, epochs = 10, loss =  nn.MSELoss(), freq_print = 1, tag = \"\"):\n",
    "\n",
    "    best_model_testing_error = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_mse = 0.0\n",
    "        for step, (input_batch, output_batch) in enumerate(training_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output_pred_batch = model(input_batch)\n",
    "            loss_f = loss(output_pred_batch, output_batch)\n",
    "            loss_f.backward()\n",
    "            optimizer.step()\n",
    "            train_mse += loss_f.item()\n",
    "        train_mse /= len(training_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_relative_l2 = 0.0\n",
    "            for step, (input_batch, output_batch) in enumerate(val_loader):\n",
    "                output_pred_batch = model(input_batch)\n",
    "                #loss_f = (torch.mean((abs(output_pred_batch - output_batch))) / torch.mean(abs(output_batch))) * 100\n",
    "                #loss_f = torch.mean(100* torch.norm(output_pred_batch - output_batch, p=2, dim = [1])/torch.norm(output_batch, p=2,dim = [1]))\n",
    "                loss_f = torch.mean(torch.norm(output_pred_batch - output_batch, p=2, dim = [1]))\n",
    "\n",
    "                test_relative_l2 += loss_f.item()\n",
    "            test_relative_l2 /= len(val_loader)\n",
    "\n",
    "\n",
    "        if test_relative_l2 < best_model_testing_error:\n",
    "            best_model_testing_error = test_relative_l2\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(best_model, f\"saved_models/model_1d_{tag}.pkl\")\n",
    "\n",
    "        if epoch % freq_print == 0: print(\"######### Epoch:\", epoch, \" ######### Train Loss:\", train_mse, \" ######### Relative L1 Test Norm:\", test_relative_l2)\n",
    "\n",
    "    return best_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xitorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_sine(x):\n",
    "    #return x\n",
    "    return np.where(x < 0, np.sin(np.pi*x/2), np.sin(25*np.pi * x))\n",
    "\n",
    "def get_dataset(N, batch_size=64):\n",
    "    X = np.random.uniform(-1,1,N)\n",
    "    Y = f_sine(X).reshape(-1,1)\n",
    "    X = X.reshape(-1,1)\n",
    "    return torch.tensor(X).to(\"cuda\").type(torch.float32), torch.tensor(Y).to(\"cuda\").type(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-1,1,1000)\n",
    "plt.scatter(X, f_sine(X), s = 12)\n",
    "plt.plot(X, f_sine(X))\n",
    "plt.grid()\n",
    "\n",
    "#plt.savefig(\"Figures/1d_diffusion/sines/true_fn.pdf\", dpi = 400)\n",
    "#plt.savefig(\"Figures/1d_diffusion/sines/true_fn.png\", dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "N_train = 5000\n",
    "N_val = 1024\n",
    "N_test = 1024\n",
    "\n",
    "\n",
    "inp_train, out_train = get_dataset(N_train, batch_size=batch_size)\n",
    "inp_val, out_val = get_dataset(N_val, batch_size=batch_size)\n",
    "inp_test, out_test = get_dataset(N_test, batch_size=batch_size)\n",
    "\n",
    "#-------------\n",
    "\n",
    "dataset_train = TensorDataset(inp_train, out_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataset_val = TensorDataset(inp_val, out_val)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0\n",
    "step_size = 100\n",
    "gamma = 0.8\n",
    "epochs = 500\n",
    "\n",
    "tag = \"tmp\"\n",
    "\n",
    "model = MLP(widths = [64, 64]).to(\"cuda\")\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "#model = train(model, optimizer, scheduler, dataloader_train, dataloader_val, epochs = epochs, freq_print=50, tag = tag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inp_test, out_test = get_dataset(5000)\n",
    "plt.plot(X, f_sine(X), label = \"True\")\n",
    "\n",
    "Y_pred = model(inp_test)[:,0]\n",
    "plt.scatter(inp_test[:,0].detach().cpu().numpy(), Y_pred.detach().cpu().numpy(), color = \"orange\", label = \"Prediction (MLP)\", s = 15)\n",
    "\n",
    "plt.legend(fontsize = 12)\n",
    "plt.grid(True)\n",
    "\n",
    "#plt.savefig(\"Figures/1d_diffusion/sines/predicted_reg_fn.pdf\", dpi = 400)\n",
    "#plt.savefig(\"Figures/1d_diffusion/sines/predicted_reg_fn.png\", dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_prob_std_1(t, sigma, device = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "    Returns:\n",
    "    The standard deviation.\n",
    "    \"\"\"\n",
    "    t = torch.tensor(t, device=device)\n",
    "    return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
    "\n",
    "def diffusion_coeff_1(t, sigma, device = \"cuda\"):\n",
    "    \"\"\"Compute the diffusion coefficient of our SDE.\n",
    "\n",
    "    Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "    Returns:\n",
    "    The vector of diffusion coefficients.\n",
    "    \"\"\"\n",
    "    return torch.tensor(sigma**t, device=device)\n",
    "\n",
    "def marginal_prob_std_2(t, sigma_min, sigma_max, device = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "    Returns:\n",
    "    The standard deviation.\n",
    "    \"\"\"\n",
    "    t = torch.tensor(t, device=device)\n",
    "    #print(torch.sqrt(sigma_min * torch.pow(sigma_max/sigma_min, t)))\n",
    "    return sigma_min * torch.pow(sigma_max/sigma_min, t)\n",
    "\n",
    "def diffusion_coeff_2(t, sigma_min, sigma_max, device = \"cuda\"):\n",
    "    \"\"\"Compute the diffusion coefficient of our SDE.\n",
    "\n",
    "    Args:\n",
    "    t: A vector of time steps.\n",
    "    sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "    Returns:\n",
    "    The vector of diffusion coefficients.\n",
    "    \"\"\"\n",
    "    return torch.tensor(sigma_min * torch.pow(sigma_max/sigma_min, t) * np.sqrt(2*np.log(sigma_max/sigma_min)), device=device)\n",
    "\n",
    "#----------------------\n",
    "\n",
    "class MLPDiffusion(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim = 1,\n",
    "                 out_dim = 1,\n",
    "                 cond_dim = 0,\n",
    "                 widths = [64, 128, 128, 64],\n",
    "                 marginal_prob_std = None):\n",
    "        super(MLPDiffusion, self).__init__()\n",
    "        \n",
    "        widths = [in_dim + 1 + cond_dim] + list(widths) + [out_dim]\n",
    "        self.n_layers = len(widths) - 1\n",
    "        self.layers = nn.ModuleList([nn.Linear(widths[i], widths[i+1]) for i in range(self.n_layers)])\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "        self.marginal_prob_std = marginal_prob_std\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                x,\n",
    "                x_cond,\n",
    "                diffusion_time):\n",
    "        \n",
    "        sigma = self.marginal_prob_std(diffusion_time)\n",
    "        #sigma = torch.log(sigma) + 1/2.\n",
    "\n",
    "        if sigma.dim() < 1:\n",
    "            sigma = sigma.expand(x.shape[0])\n",
    "        \n",
    "        if x_cond is not None:\n",
    "            x = torch.cat((x, x_cond, torch.log(sigma)[:, None]), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((x, torch.log(sigma)[:, None]), dim=1)\n",
    "        \n",
    "        \n",
    "        for i in range(self.n_layers - 1):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.activation(x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x/sigma[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, \n",
    "            x, \n",
    "            x_cond,\n",
    "            marginal_prob_std = None, \n",
    "            eps=1e-6,\n",
    "            is_train = True):\n",
    "\n",
    "        if is_train:\n",
    "                random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps    \n",
    "                z = torch.randn_like(x, device = x.device)    \n",
    "                std = marginal_prob_std(random_t) \n",
    "                #score = model(x + z * std[:, None], x_cond, random_t)\n",
    "                #loss = torch.mean(torch.mean((score * std[:, None] + z)**2, dim=(1)))\n",
    "                \n",
    "                denoised = model(x + z * std[:, None], x_cond, random_t)\n",
    "\n",
    "                x_squared = torch.square(x)\n",
    "                denoise_squared = torch.square(denoised)\n",
    "                loss = torch.mean(torch.square(denoised - x))\n",
    "                return loss\n",
    "        else:\n",
    "                err_val = 0\n",
    "                for level in range(8):\n",
    "                        t_min = level/8.0\n",
    "                        t_max = (level + 1)/8.0\n",
    "                        random_t = (torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps )*(t_max-t_min)+t_min\n",
    "                        \n",
    "                        z = torch.randn_like(x)\n",
    "                        std = marginal_prob_std(random_t) \n",
    "\n",
    "                        denoised = model(x + z * std[:, None], x_cond, random_t)\n",
    "                        loss = torch.mean(torch.square(denoised - x))\n",
    "                        err_val = err_val + loss\n",
    "                return err_val/8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion.model import EMA\n",
    "\n",
    "def Euler_Maruyama_sampler(model,\n",
    "                           marginal_prob_std,\n",
    "                           diffusion_coeff,\n",
    "                           condition,\n",
    "                           batch_size=64,\n",
    "                           num_steps=128,\n",
    "                           device='cuda',\n",
    "                           dimension = (2,),\n",
    "                           eps=1e-4):\n",
    "  \"\"\"Generate samples from score-based models with the Euler-Maruyama solver.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model that represents the time-dependent score-based model.\n",
    "    marginal_prob_std: A function that gives the standard deviation of\n",
    "      the perturbation kernel.\n",
    "    diffusion_coeff: A function that gives the diffusion coefficient of the SDE.\n",
    "    batch_size: The number of samplers to generate by calling this function once.\n",
    "    num_steps: The number of sampling steps.\n",
    "      Equivalent to the number of discretized time steps.\n",
    "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
    "    eps: The smallest time step for numerical stability.\n",
    "\n",
    "  Returns:\n",
    "    Samples.\n",
    "  \"\"\"\n",
    "\n",
    "  device = next(model.parameters()).device\n",
    "\n",
    "  t = torch.ones(batch_size, device=device)\n",
    "  init_x = torch.randn(batch_size, dimension[0], device=device) * marginal_prob_std(t).to(device)[:, None]\n",
    "  time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
    "  step_size = time_steps[0] - time_steps[1]\n",
    "  x = init_x\n",
    "  with torch.no_grad():\n",
    "    for time_step in tqdm.tqdm(time_steps):\n",
    "        batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
    "        g = diffusion_coeff(batch_time_step).to(device)\n",
    "        #score = model(x, condition, batch_time_step)\n",
    "        denoised = model(x, condition, batch_time_step)\n",
    "        std = marginal_prob_std(batch_time_step)\n",
    "        score = (denoised - x)/std[:, None]**2\n",
    "        mean_x = x + (g**2)[:, None] * score * step_size\n",
    "        x = mean_x + torch.sqrt(step_size) * g[:, None] * torch.randn_like(x)\n",
    "        \n",
    "  return mean_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_sampler(score_model,\n",
    "                marginal_prob_std,\n",
    "                diffusion_coeff,\n",
    "                condition = None,\n",
    "                batch_size=64,\n",
    "                atol=1e-5,\n",
    "                rtol=1e-5,\n",
    "                device='cuda',\n",
    "                z=None,\n",
    "                dimension = (2,),\n",
    "                eps=1e-3):\n",
    "  \"\"\"Generate samples from score-based models with black-box ODE solvers.\n",
    "\n",
    "  Args:\n",
    "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
    "    marginal_prob_std: A function that returns the standard deviation\n",
    "      of the perturbation kernel.\n",
    "    diffusion_coeff: A function that returns the diffusion coefficient of the SDE.\n",
    "    batch_size: The number of samplers to generate by calling this function once.\n",
    "    atol: Tolerance of absolute errors.\n",
    "    rtol: Tolerance of relative errors.\n",
    "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
    "    z: The latent code that governs the final sample. If None, we start from p_1;\n",
    "      otherwise, we start from the given z.\n",
    "    eps: The smallest time step for numerical stability.\n",
    "  \"\"\"\n",
    "\n",
    "  t = torch.ones(batch_size, device=device)\n",
    "  # Create the latent code\n",
    "  if z is None:\n",
    "    init_x = torch.randn(batch_size, dimension[0], device=device) * marginal_prob_std(t)[:, None]\n",
    "  else:\n",
    "    init_x = z\n",
    "\n",
    "  shape = init_x.shape\n",
    "\n",
    "  def score_eval_wrapper(sample, time_steps):\n",
    "    \"\"\"A wrapper of the score-based model for use by the ODE solver.\"\"\"\n",
    "    score_model.train()\n",
    "    sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n",
    "    time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))\n",
    "    denoised = score_model(sample, None, time_steps)\n",
    "    std = marginal_prob_std(time_steps)\n",
    "    \n",
    "    #score = (denoised - sample)/std[:, None]**2\n",
    "    \n",
    "    return ((denoised - sample)/std[:, None]**2).detach().cpu().numpy().reshape((-1))\n",
    "\n",
    "  def ode_func(t, x):\n",
    "    \"\"\"The ODE function for use by the ODE solver.\"\"\"\n",
    "    time_steps = np.ones((shape[0],)) * t\n",
    "    g = diffusion_coeff(torch.tensor(t)).cpu().numpy()\n",
    "    return  -0.5 * (g**2) * score_eval_wrapper(x, time_steps)\n",
    "\n",
    "  # Run the black-box ODE solver.\n",
    "  res = integrate.solve_ivp(ode_func, (1., eps), init_x.reshape(-1).cpu().numpy(), rtol=rtol, atol=atol, method='RK45')\n",
    "  print(f\"Number of function evaluations: {res.nfev}\")\n",
    "  x = torch.tensor(res.y[:, -1], device=device).reshape(shape)\n",
    "\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import vmap, jvp\n",
    "\n",
    "def prior_likelihood(z, sigma):  \n",
    "    \"\"\"The likelihood of a Gaussian distribution with mean zero and\n",
    "      standard deviation sigma.\"\"\"\n",
    "    shape = z.shape\n",
    "    N = np.prod(shape[1:])\n",
    "    return -N / 2. * np.log(2*np.pi*sigma**2) - np.sum(z**2, axis=(1,)) / (2 * sigma**2)\n",
    "\n",
    "\n",
    "def ode_likelihood(model,\n",
    "                   x,\n",
    "                   condition,\n",
    "                   marginal_prob_std,\n",
    "                   diffusion_coeff,\n",
    "                   t_batch = None,\n",
    "                   batch_size=64,\n",
    "                   device='cuda',\n",
    "                   eps = 1e-6,\n",
    "                   rtol = 1e-6,\n",
    "                   atol = 1e-6,\n",
    "                   epsilon = None):\n",
    "\n",
    "    shape = x.shape\n",
    "    B,C = shape[0], np.prod(shape[1:])\n",
    "    shape_epsilon = (batch_size,)+shape[1:] \n",
    "    #epsilon = torch.randn(shape_epsilon, device = device).type(torch.float32) #Shape (batchsize, other)\n",
    "    #epsilon = torch.sqrt(torch.prod(torch.tensor(shape[1:], device=device))) * epsilon / torch.norm(epsilon, dim=1, keepdim=True)\n",
    "\n",
    "    \"\"\"def divergence_eval(fn, x, epsilon_all):\n",
    "        '''\n",
    "        Args:\n",
    "            fn: Python function mapping tensor input x to tensor output (output.shape==x.shape)\n",
    "            x: torch.Tensor of shape (B,other), x is the point at which divergence is estimated\n",
    "            epsilon_all: torch.Tensor of shape (sample_size,other), samples for Hutchinson estimator\n",
    "\n",
    "        Returns:\n",
    "            div_f_estimates: torch.Tensor of shape (B, sample_size), contains values\n",
    "                            epsilon_all[j]^T . Jfn(x[i]) . epsilon_all[j]\n",
    "        '''\n",
    "        # Define the JVP computation\n",
    "        \n",
    "        def e_Jf_e(eps):\n",
    "            _, jvp_result = jvp(f_e, (x, eps), (eps.repeat(B,1), torch.zeros_like(eps)))\n",
    "            return jvp_result\n",
    "\n",
    "        #def f_e(x, eps):\n",
    "        #    return torch.sum(fn(x).reshape(B,C) * eps.view(1,-1), dim=-1)\n",
    "\n",
    "        #x = torch.tensor(x,  device=device, dtype=torch.float32)\n",
    "        #x.requires_grad_(True)\n",
    "        #epsilon_all.requires_grad_(True)\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "\n",
    "            '''\n",
    "            print(\"X.requires_grad:\", x.requires_grad)  \n",
    "            print(\"Epsilon shape:\", epsilon_all.shape)\n",
    "            print(\"Function output shape:\", fn(x).shape)\n",
    "            print(\"fn(x) requires grad:\", fn(x).requires_grad)\n",
    "            '''\n",
    "            # define relevant functions and gradients\n",
    "            f_e = lambda x, eps: torch.sum(fn(x).reshape(B,C) * eps.view(1,-1), dim=-1) # shape (B,)\n",
    "            e_Jf_e = lambda eps: jvp(f_e, (x, eps), (eps.repeat(B,1), torch.zeros_like(eps)))[1]\n",
    "            e_Jf_e_multisample = vmap(e_Jf_e, out_dims=1) # allow additional \"batch-dimension\" for eps (this is sample_size)\n",
    "\n",
    "            # get the estimates\n",
    "            #print(epsilon_all)\n",
    "            div_f_estimates = e_Jf_e_multisample(epsilon_all) # x: (B, other), epsilon_all: (sample_size, other)\n",
    "            #print(div_f_estimates)\n",
    "            return div_f_estimates.reshape(-1,)\"\"\"\n",
    "\n",
    "    def divergence_eval(fn, x, epsilon):\n",
    "        \"\"\"Compute the divergence of the score-based model with Skilling-Hutchinson.\"\"\"\n",
    "        \n",
    "        #with torch.enable_grad():\n",
    "\n",
    "        #res = torch.zeros((B,batch_size), device = device).type(torch.float32)\n",
    "        #for i in range(batch_size):\n",
    "        epsilon.requires_grad_(True)\n",
    "        x.requires_grad_(True)\n",
    "        f_e = lambda x, eps: torch.sum(fn(x).reshape(B,C) * eps.view(1,-1), dim=-1) # shape (B,)\n",
    "        #e_Jf_e = lambda eps: torch.autograd.functional.jvp(f_e, (x, eps), (eps.repeat(B,1), torch.zeros_like(eps)))[1]\n",
    "        #res[:,i] =  e_Jf_e(epsilon[i])\n",
    "\n",
    "        def e_Jf_e(eps):\n",
    "            _, jvp_result = jvp(f_e, (x, eps), (eps.repeat(B,1), torch.zeros_like(eps)))\n",
    "            return jvp_result\n",
    "\n",
    "        e_Jf_e_multisample = vmap(e_Jf_e, out_dims=1) # allow additional \"batch-dimension\" for eps (this is sample_size)\n",
    "        #res = torch.mean(res, dim = 1)\n",
    "        res = e_Jf_e_multisample(epsilon)\n",
    "        return res.mean(dim=1)\n",
    "\n",
    "    def ode_func(t, x):\n",
    "        \"\"\"The ODE function for the black-box solver.\"\"\"\n",
    "        time_steps = np.ones((shape[0],)) * t   \n",
    "        sample = x[:B*C]\n",
    "        logp = x[B*C:]\n",
    "        g = diffusion_coeff(torch.tensor(t))\n",
    "        \n",
    "        sample = torch.as_tensor(sample, device=device, dtype=torch.float32).reshape(shape)  # Convert to tensor\n",
    "        \n",
    "        time_steps = time_steps.reshape((sample.shape[0],))\n",
    "        \n",
    "\n",
    "        time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32)\n",
    "        sample = torch.tensor(sample, device=device, dtype=torch.float32)\n",
    "        std = marginal_prob_std(time_steps)[:, None]\n",
    "\n",
    "        #sample.requires_grad_(True)\n",
    "        #epsilon.requires_grad_(True)\n",
    "        #epsilon = torch.randn((batch_size, 2), device = device).type(torch.float32)\n",
    "        #epsilon = torch.sqrt(torch.prod(torch.tensor(2, device=device))) * epsilon / torch.norm(epsilon, dim=1, keepdim=True)\n",
    "        model.train()\n",
    "        fn = lambda x: (model(x, condition, time_steps.reshape((B,))) - x.reshape(shape)) / std**2\n",
    "\n",
    "\n",
    "        logp_grad = -0.5 * g**2 * divergence_eval(fn, sample, epsilon)\n",
    "        sample_grad = -0.5 * g**2 * fn(sample)\n",
    "        \n",
    "        return np.concatenate([sample_grad.reshape(-1).detach().cpu().numpy(), logp_grad.reshape(-1).detach().cpu().numpy()], axis=0)\n",
    "\n",
    "    init = np.concatenate([x.cpu().numpy().reshape((-1,)), np.zeros((shape[0],))], axis=0)\n",
    "\n",
    "    # Black-box ODE solver\n",
    "    res = integrate.solve_ivp(ode_func, (eps, 1.), init, rtol=rtol, atol=atol, method='RK45')\n",
    "    \n",
    "    zp = res.y[:,-1]\n",
    "\n",
    "    z, delta_logp = zp[:B*C], zp[B*C:]\n",
    "    z = z.reshape((B,C))\n",
    "\n",
    "    sigma_max = marginal_prob_std(torch.tensor(1.0, device=device)).detach().cpu().numpy()\n",
    "    prior_logp = prior_likelihood(z, sigma_max)\n",
    "\n",
    "    N = np.prod(shape[1:])\n",
    "    \n",
    "    return z, prior_logp/N, delta_logp/N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import vmap, jvp\n",
    "from xitorch import integrate as integrate_torch\n",
    "\n",
    "def prior_likelihood_torch(z, sigma):  \n",
    "    \"\"\"The likelihood of a Gaussian distribution with mean zero and\n",
    "      standard deviation sigma.\"\"\"\n",
    "    shape = z.shape\n",
    "    N = torch.prod(torch.tensor(shape[1:], device=device))\n",
    "    dim = [i for i in range(1, len(shape))]\n",
    "    return -N / 2. * torch.log(2*np.pi*sigma**2) - torch.sum(z**2, dim=dim) / (2 * sigma**2)\n",
    "\n",
    "\n",
    "def ode_likelihood_torch(model,\n",
    "                        x,\n",
    "                        condition,\n",
    "                        marginal_prob_std,\n",
    "                        diffusion_coeff,\n",
    "                        t_batch = None,\n",
    "                        batch_size=64,\n",
    "                        device='cuda',\n",
    "                        eps = 1e-6,\n",
    "                        rtol = 1e-6,\n",
    "                        atol = 1e-6,\n",
    "                        epsilon = None):\n",
    "\n",
    "    shape = x.shape\n",
    "    B,C = shape[0], torch.tensor(shape[1:], device=device)\n",
    "    shape_epsilon = (batch_size,)+shape[1:] \n",
    "\n",
    "    def divergence_eval(fn, x, epsilon):\n",
    "        \"\"\"Compute the divergence of the score-based model with Skilling-Hutchinson.\"\"\"\n",
    "\n",
    "        epsilon.requires_grad_(True)\n",
    "        x.requires_grad_(True)\n",
    "        f_e = lambda x, eps: torch.sum(fn(x).reshape(B,C) * eps.view(1,-1), dim=-1) # shape (B,)\n",
    "\n",
    "        def e_Jf_e(eps):\n",
    "            _, jvp_result = jvp(f_e, (x, eps), (eps.repeat(B,1), torch.zeros_like(eps)))\n",
    "            return jvp_result\n",
    "\n",
    "        e_Jf_e_multisample = vmap(e_Jf_e, out_dims=1) # allow additional \"batch-dimension\" for eps (this is sample_size)\n",
    "        res = e_Jf_e_multisample(epsilon)\n",
    "        return res.mean(dim=1)\n",
    "\n",
    "    def ode_func(t, x):\n",
    "        \"\"\"The ODE function for the black-box solver.\"\"\"\n",
    "        time_steps = torch.ones((shape[0],), device = device) * t   \n",
    "        sample = x[:B*C]\n",
    "        logp = x[B*C:]\n",
    "        g = diffusion_coeff(torch.tensor(t, device=device))\n",
    "        \n",
    "        sample = sample.reshape(shape)  # Convert to tensor\n",
    "        time_steps = time_steps.reshape((sample.shape[0],))\n",
    "\n",
    "        std = marginal_prob_std(time_steps)[(slice(None),) + (None,) * (len(shape) - 1)]\n",
    "        model.train()\n",
    "        fn = lambda x: (model(x, condition, time_steps.reshape((B,))) - x.reshape(shape)) / std**2\n",
    "\n",
    "        logp_grad = -0.5 * g**2 * divergence_eval(fn, sample, epsilon)\n",
    "        sample_grad = -0.5 * g**2 * fn(sample)\n",
    "        \n",
    "        return torch.cat([sample_grad.reshape(-1), logp_grad.reshape(-1)], axis = 0)\n",
    "\n",
    "    '''\n",
    "    init = np.concatenate([x.cpu().numpy().reshape((-1,)), np.zeros((shape[0],))], axis=0)\n",
    "    # Black-box ODE solver\n",
    "    res = integrate.solve_ivp(ode_func, (eps, 1.), init, rtol=rtol, atol=atol, method='RK45')\n",
    "    zp = res.y[:,-1]\n",
    "    '''\n",
    "    init = torch.cat([x.view(-1), torch.zeros(shape[0], device=device)])\n",
    "    #res = odeint(ode_func, init, torch.tensor([eps, 1.0], device=device), rtol=rtol, atol=atol, method='heun2') #heun2 #rk4\n",
    "    \n",
    "    res = integrate_torch.solve_ivp(fcn = ode_func, y0=init, ts = torch.tensor([eps, 1.0], device=device), rtol=rtol, atol=atol, method=\"rk38\")\n",
    "    zp = res[-1]\n",
    "    \n",
    "    z, delta_logp = zp[:B*C], zp[B*C:]\n",
    "    z = z.reshape((B,C))\n",
    "\n",
    "    sigma_max = marginal_prob_std(torch.tensor(1.0, device=device))\n",
    "    prior_logp = prior_likelihood_torch(z, sigma_max)\n",
    "\n",
    "    N = torch.prod(torch.tensor(shape[1:], device=device))\n",
    "    \n",
    "    return z, prior_logp/N, delta_logp/N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "step_size = 50\n",
    "gamma = 0.95\n",
    "epochs = 1\n",
    "device = \"cuda\"\n",
    "\n",
    "tqdm_epoch = tqdm.trange(epochs)\n",
    "\n",
    "save_epochs = [100, 1000, 2000, 5000, 9999]\n",
    "\n",
    "which_type = \"x&y\"\n",
    "if which_type == \"x&y\":\n",
    "  cond_dim = 0\n",
    "  in_dim = 2\n",
    "else:\n",
    "  cond_dim = 1\n",
    "  in_dim = 1\n",
    "\n",
    "sigma = 100.\n",
    "\n",
    "#marginal_prob_std_fn = functools.partial(marginal_prob_std_1, sigma=sigma, device = device)\n",
    "#diffusion_coeff_fn = functools.partial(diffusion_coeff_1, sigma=sigma, device = device)\n",
    "\n",
    "marginal_prob_std_fn = functools.partial(marginal_prob_std_2, sigma_min = 0.001, sigma_max=sigma, device = device)\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff_2, sigma_min = 0.001, sigma_max=sigma, device = device)\n",
    "\n",
    "model_diff = MLPDiffusion(widths = [1024, 1024, 1024], marginal_prob_std = marginal_prob_std_fn, cond_dim=cond_dim, in_dim=in_dim, out_dim=in_dim).to(\"cuda\")\n",
    "optimizer = AdamW(model_diff.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "best_avg_loss = 1000.\n",
    "best_model_diff = None\n",
    "\n",
    "for epoch in tqdm_epoch:\n",
    "    avg_loss = 0.\n",
    "    num_items = 0\n",
    "    avg_loss_val = 0.\n",
    "\n",
    "    model_diff.train()\n",
    "\n",
    "    for input_batch, output_batch in dataloader_train:\n",
    "      input_batch = input_batch.to(device)\n",
    "      output_batch = output_batch.to(device)\n",
    "\n",
    "      if which_type == \"yx\":\n",
    "        loss = loss_fn(model_diff, output_batch, input_batch, marginal_prob_std_fn)\n",
    "        condition = input_batch\n",
    "      elif which_type == \"x&y\":\n",
    "        loss = loss_fn(model_diff, torch.cat((input_batch, output_batch), axis = 1), None, marginal_prob_std_fn)\n",
    "        condition = None\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      avg_loss += loss.item() * output_batch.shape[0]\n",
    "      num_items += output_batch.shape[0]\n",
    "    avg_loss /= num_items\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model_diff.eval()\n",
    "    \n",
    "    num_items = 0\n",
    "    with torch.no_grad():\n",
    "        for input_batch, output_batch in dataloader_val:\n",
    "          input_batch = input_batch.to(device)\n",
    "          output_batch = output_batch.to(device)\n",
    "\n",
    "          if which_type == \"yx\":\n",
    "            loss = loss_fn(model_diff, output_batch, input_batch, marginal_prob_std_fn, is_train = False)\n",
    "          elif which_type == \"x&y\":\n",
    "            loss = loss_fn(model_diff, torch.cat((input_batch, output_batch), axis = 1), None, marginal_prob_std_fn, is_train = False)\n",
    "          avg_loss_val += loss.item() * output_batch.shape[0]\n",
    "          num_items += output_batch.shape[0]\n",
    "    avg_loss_val/= num_items\n",
    "\n",
    "    \n",
    "    if avg_loss_val<best_avg_loss:\n",
    "      best_avg_loss = avg_loss_val    \n",
    "      best_model_diff = copy.deepcopy(model_diff)\n",
    "    \n",
    "\n",
    "    if epoch in save_epochs:\n",
    "      torch.save(best_model_diff.state_dict(), f\"saved_models/diff_model1d_ep_{epoch}.pth\")\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "      tqdm_epoch.set_description('Train: {:.5f} Val: {:.5f}'.format(avg_loss, avg_loss_val))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 100.\n",
    "cond_dim = 0\n",
    "in_dim = 2\n",
    "marginal_prob_std_fn = functools.partial(marginal_prob_std_2, sigma_min = 0.001, sigma_max=sigma, device = device)\n",
    "diffusion_coeff_fn = functools.partial(diffusion_coeff_2, sigma_min = 0.001, sigma_max=sigma, device = device)\n",
    "\n",
    "path =  \"/path_to_model/\"\n",
    "device = \"cuda\"\n",
    "model_best = MLPDiffusion(widths = [1024, 1024, 1024], marginal_prob_std = marginal_prob_std_fn, cond_dim=cond_dim, in_dim=in_dim, out_dim=in_dim).to(\"cuda\")\n",
    "model_best.load_state_dict(torch.load(path, map_location=device, weights_only=True))\n",
    "model_best.eval()\n",
    "\n",
    "\n",
    "samples_sde = Euler_Maruyama_sampler(model_best,\n",
    "                                marginal_prob_std_fn,\n",
    "                                diffusion_coeff_fn,\n",
    "                                None,\n",
    "                                batch_size=2048,\n",
    "                                num_steps=512,\n",
    "                                device='cuda',\n",
    "                                dimension = (2,),\n",
    "                                eps=1e-10)\n",
    "\n",
    "samples_ode = ode_sampler(model_best,\n",
    "                marginal_prob_std_fn,\n",
    "                diffusion_coeff_fn,\n",
    "                condition = None,\n",
    "                batch_size=1024,\n",
    "                atol=1e-6,\n",
    "                rtol=1e-6,\n",
    "                device='cuda',\n",
    "                z=None,\n",
    "                dimension = (2,),\n",
    "                eps=1e-10)\n",
    "\n",
    "plt.scatter(samples_ode[:,0].detach().cpu().numpy(),samples_ode[:,1].detach().cpu().numpy(), color = \"blue\", s = 16, label = \"Sampled (ODE)\")\n",
    "X = np.linspace(-1,1.05,1000)\n",
    "plt.plot(X, f_sine(X), \"--\", color = \"red\", label = \"Ground Truth\")\n",
    "plt.grid()\n",
    "\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([-1.05,1.05])\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"x\",fontsize=14)\n",
    "plt.ylabel(\"f(x)\",fontsize=14)\n",
    "\n",
    "#plt.savefig(\"Figures/1d_diffusion/sines/gen_samples_ODE_fn.pdf\", dpi = 400)\n",
    "#plt.savefig(\"Figures/1d_diffusion/sines/gen_samples_ODE_fn.png\", dpi = 400)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(samples_sde[:,0].detach().cpu().numpy(),samples_sde[:,1].detach().cpu().numpy(), color = \"green\", s = 16, label = \"Sampled (SDE)\")\n",
    "X = np.linspace(-1,1.05,1000)\n",
    "plt.plot(X, f_sine(X), \"--\", color = \"red\", label = \"Ground Truth\")\n",
    "plt.grid()\n",
    "\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([-1.05,1.05])\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"x\",fontsize=14)\n",
    "plt.ylabel(\"f(x)\",fontsize=14)\n",
    "\n",
    "plt.savefig(\"Figures/1d_diffusion/sines/gen_samples_SDE_fn.pdf\", dpi = 400)\n",
    "plt.savefig(\"Figures/1d_diffusion/sines/gen_samples_SDE_fn.png\", dpi = 400)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(coordinates[:,0].detach().cpu().numpy(), coordinates[:,1].detach().cpu().numpy(), s = 0.25, label = \"Target Points\")\n",
    "plt.plot([0,0],[-l- 0.05,l+ 0.05], color = \"blue\")\n",
    "plt.plot([-l- 0.05,l+ 0.05],[0,0], color = \"blue\")\n",
    "plt.xlim([-l - 0.05, l + 0.05])\n",
    "plt.ylim([-l - 0.05, l + 0.05])\n",
    "plt.legend(fontsize = 12)\n",
    "\n",
    "plt.xlabel(\"x coordinate\", fontsize = 12)\n",
    "plt.ylabel(\"y coordinate\", fontsize = 12)\n",
    "    \n",
    "#plt.savefig(\"Figures/1d_diffusion/sines/target_points_likelihood.pdf\", dpi = 400)\n",
    "#plt.savefig(\"Figures/1d_diffusion/sines/target_points_likelihood.png\", dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "S = 128  # Example resolution\n",
    "step_size = (1/(S))\n",
    "\n",
    "# Generate 1D grid points\n",
    "l = 1.0\n",
    "x = np.arange(-l, l, step_size*(l))\n",
    "y = np.arange(-l, l, step_size*(l))\n",
    "\n",
    "# Create 2D grid\n",
    "xx, yy = np.meshgrid(x, y, indexing='ij')\n",
    "print(xx.shape)\n",
    "# Stack coordinates\n",
    "coordinates = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "coordinates = torch.tensor(coordinates).to(\"cuda\").type(torch.float32)\n",
    "\n",
    "path = \"/path_to_model/\"\n",
    "device = \"cuda\"\n",
    "model_best = MLPDiffusion(widths = [1024, 1024, 1024], marginal_prob_std = marginal_prob_std_fn, cond_dim=cond_dim, in_dim=in_dim, out_dim=in_dim).to(\"cuda\")\n",
    "model_best.load_state_dict(torch.load(path, map_location=device, weights_only=True))\n",
    "\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(coordinates)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "\n",
    "batch_size = 256\n",
    "epsilon = torch.randn((batch_size, 2), device = device).type(torch.float32)\n",
    "epsilon = torch.sqrt(torch.prod(torch.tensor(2, device=device))) * epsilon / torch.norm(epsilon, dim=1, keepdim=True)\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(i, len(dataloader))\n",
    "    x_, prior_, delta_ = ode_likelihood_torch(model= model_best,\n",
    "                                            x = batch[0],\n",
    "                                            condition = None,\n",
    "                                            marginal_prob_std = marginal_prob_std_fn,\n",
    "                                            diffusion_coeff = diffusion_coeff_fn,\n",
    "                                            batch_size = batch_size,\n",
    "                                            device='cuda',\n",
    "                                            eps = 1e-8,\n",
    "                                            rtol = 1e-2,\n",
    "                                            atol = 1e-2,\n",
    "                                            epsilon = epsilon)\n",
    "                        \n",
    "    if i == 0:\n",
    "        x, prior, delta = x_.detach().cpu().numpy(), prior_.detach().cpu().numpy(), delta_.detach().cpu().numpy()\n",
    "    else:\n",
    "        x = np.concatenate((x, x_.detach().cpu().numpy()))\n",
    "        prior = np.concatenate((prior, prior_.detach().cpu().numpy()))\n",
    "        delta = np.concatenate((delta, delta_.detach().cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xx.shape)\n",
    "xx =xx.reshape((2*S, 2*S))\n",
    "mask_neg = np.logical_and(xx < 0, np.abs(f_sine(xx)-yy)<1000)\n",
    "mask_neg = mask_neg.reshape(-1)\n",
    "mask_pos = xx > 0 \n",
    "mask_pos = mask_pos.reshape(-1)\n",
    "\n",
    "X = x[:,0]/sigma\n",
    "Y = x[:,1]/sigma\n",
    "\n",
    "for i,mask in enumerate([mask_pos, mask_neg]):\n",
    "    Y_plt = Y[mask]\n",
    "    X_plt = X[mask]\n",
    "    \n",
    "    bins = 100\n",
    "    plt.hist2d(X_plt,Y_plt, bins = bins, density= True, cmap = \"gist_ncar\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.xlabel(\"x coordinate of the prior\", fontsize = 12)\n",
    "    plt.ylabel(\"y coordinate of the prior\", fontsize = 12)\n",
    "    \n",
    "    '''\n",
    "    if i ==0:\n",
    "        plt.savefig(\"Figures/1d_diffusion/sines/prior_hist_positive_x.pdf\", dpi = 400)\n",
    "        plt.savefig(\"Figures/1d_diffusion/sines/prior_hist_positive_x.png\", dpi = 400)\n",
    "    elif i ==1:\n",
    "        plt.savefig(\"Figures/1d_diffusion/sines/prior_hist_negative_x.pdf\", dpi = 400)\n",
    "        plt.savefig(\"Figures/1d_diffusion/sines/prior_hist_negative_x.png\", dpi = 400)\n",
    "    '''\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "Y_pos = Y[mask_pos]\n",
    "X_pos = X[mask_pos]\n",
    "Y_neg = Y[mask_neg]\n",
    "X_neg = X[mask_neg]\n",
    "\n",
    "mask_neg = np.logical_and(np.abs(X_neg)<np.max(X_pos), np.abs(Y_neg)<np.max(Y_pos))\n",
    "Y_neg = Y_neg[mask_neg]\n",
    "X_neg = X_neg[mask_neg]\n",
    "\n",
    "X_plt = np.concatenate((X_pos,X_neg))\n",
    "Y_plt = np.concatenate((Y_pos,Y_neg))\n",
    "\n",
    "bins = 75\n",
    "plt.hist2d(X_plt,Y_plt, bins = bins, density= True, cmap = \"gist_ncar\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xlabel(\"x coordinate of the prior\", fontsize = 12)\n",
    "plt.ylabel(\"y coordinate of the prior\", fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
